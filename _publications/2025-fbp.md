---
title: "Frozen Backpropagation: Relaxing Weight Symmetry in Temporally-Coded Deep Spiking Neural Networks"
collection: publications
permalink: /publication/2025-fbp
excerpt: #'This paper is about the number 1. The number 2 is left for future work.'
date: 2025-05-19
#category: conferences
venue: 'arXiv' #'Advances in Neural Information Processing Systems'
slidesurl: #'http://academicpages.github.io/files/slides1.pdf'
paperurl: #'https://arxiv.org/abs/2410.17066'
citation: 'Gaspard Goupy, Pierre Tirilly, and Ioan Marius Bilasco. Frozen Backpropagation: Relaxing Weight Symmetry in Temporally-Coded Deep Spiking Neural Networks.
arXiv, arXiv:2505.13741, 2025.'
---

[arXiv](https://arxiv.org/abs/2505.13741) - [Code](https://gitlab.univ-lille.fr/fox/fbp)

In this paper, we consider a dual-network configuration, using separate networks with distinct weights for the forward and backward passes.
This configuration is relevant for implementation on neuromorphic hardware but poses challenges for BP-based training, as correct credit assignment requires exact symmetry between forward and feedback weights throughout training.
We introduce the Frozen Backpropagation (FBP) algorithm to relax the weight symmetry requirement without compromising performance, enabling a more efficient implementation of BP-based training on neuromorphic hardware.

![Figure of the dual-network configuration](https://ggoupy.github.io/images/architecture_fbp.png)